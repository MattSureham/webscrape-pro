# WebScrape Pro ğŸ•¸ï¸

A comprehensive, production-ready web scraping toolkit with multiple engines, advanced features, and extensible architecture.

## âœ¨ Features

### Scraping Engines
- **SmartScraper** - Requests + BeautifulSoup with caching, retries, proxies
- **AsyncScraper** - aiohttp for high-performance concurrent scraping
- **SeleniumScraper** - Chrome/Firefox automation for JavaScript-heavy sites
- **PlaywrightScraper** - Modern browser automation with stealth capabilities

### Data Exporters
- JSON / JSONL
- CSV / TSV
- Excel (multi-sheet support)
- SQLite
- Apache Parquet
- MongoDB

### Middleware
- **Caching** - Memory and disk-based caching
- **Retry Logic** - Exponential backoff with jitter
- **Rate Limiting** - Token bucket and sliding window algorithms

### Utilities
- HTML parsing (tables, forms, links, metadata extraction)
- URL manipulation and validation
- Email/phone extraction and validation

## ğŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone https://github.com/MattSureham/webscrape-pro.git
cd webscrape-pro

# Install dependencies
pip install -r requirements.txt
```

### Basic Usage

```python
from webscrape_pro import SmartScraper

# Simple scraping
scraper = SmartScraper()
soup = scraper.scrape('https://example.com')
print(soup.title.text)

# With configuration
from webscrape_pro.core.scraper import ScrapingConfig

config = ScrapingConfig(
    delay_range=(2, 5),
    use_cache=True,
    rotate_user_agents=True
)
scraper = SmartScraper(config)
```

### Async Scraping

```python
import asyncio
from webscrape_pro import AsyncScraper

async def main():
    async with AsyncScraper() as scraper:
        urls = ['https://example.com/page1', 'https://example.com/page2']
        soups = await scraper.scrape_many(urls)
        return soups

asyncio.run(main())
```

### Browser Automation

```python
from webscrape_pro import SeleniumScraper

with SeleniumScraper() as scraper:
    scraper.get('https://example.com')
    scraper.type_text('input#search', 'python')
    scraper.click('button#submit')
    html = scraper.get_page_source()
```

### Exporting Data

```python
from webscrape_pro.exporters.base import JSONExporter, CSVExporter

data = [
    {'name': 'Product A', 'price': 29.99},
    {'name': 'Product B', 'price': 39.99}
]

# Export to JSON
JSONExporter('output.json').export(data)

# Export to CSV
CSVExporter('output.csv').export(data)
```

## ğŸ“ Repository Structure

```
webscrape-pro/
â”œâ”€â”€ webscrape_pro/              # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ core/                   # Core scraping engines
â”‚   â”‚   â”œâ”€â”€ scraper.py          # SmartScraper (requests/bs4)
â”‚   â”‚   â”œâ”€â”€ async_scraper.py    # AsyncScraper (aiohttp)
â”‚   â”‚   â”œâ”€â”€ browser.py          # SeleniumScraper
â”‚   â”‚   â””â”€â”€ playwright_scraper.py # PlaywrightScraper
â”‚   â”œâ”€â”€ exporters/              # Data export modules
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ base.py             # JSON, CSV, Excel, SQLite, Parquet, MongoDB
â”‚   â”œâ”€â”€ middleware/             # Middleware components
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ cache.py            # Caching (memory/disk)
â”‚   â”‚   â”œâ”€â”€ retry.py            # Retry logic with backoff
â”‚   â”‚   â””â”€â”€ rate_limiter.py     # Rate limiting algorithms
â”‚   â””â”€â”€ utils/                  # Utility functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ parsers.py          # HTMLParser, URLParser
â”‚       â””â”€â”€ validators.py       # URL, email, phone validators
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ setup.py
```

## ğŸ“¦ Requirements

- Python 3.8+
- requests
- beautifulsoup4
- aiohttp (for async)
- selenium (optional, for browser automation)
- playwright (optional, for modern browser automation)
- pandas (for data export)
- pymongo (optional, for MongoDB export)

## ğŸ”§ Advanced Configuration

### Using Proxies

```python
config = ScrapingConfig(
    proxy_list=['http://proxy1:8080', 'http://proxy2:8080'],
    rotate_user_agents=True
)
scraper = SmartScraper(config)
```

### Custom Rate Limiting

```python
from webscrape_pro.middleware.rate_limiter import TokenBucket

bucket = TokenBucket(rate=2.0, capacity=5)  # 2 requests/sec, burst of 5
bucket.acquire()  # Wait for token
```

### Caching

```python
from webscrape_pro.middleware.cache import CacheManager

cache = CacheManager(backend='disk', cache_dir='.cache')
cache.set('key', value, ttl=3600)
value = cache.get('key')
```

## ğŸ§ª Running Tests

```bash
pytest tests/
```

## ğŸ“„ License

MIT License - see LICENSE file for details

## ğŸ™ Contributing

Contributions welcome! Please feel free to submit a Pull Request.
